{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7f7c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2900 entries, 0 to 2899\n",
      "Data columns (total 8 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Time_spent_Alone           2837 non-null   float64\n",
      " 1   Stage_fear                 2827 non-null   object \n",
      " 2   Social_event_attendance    2838 non-null   float64\n",
      " 3   Going_outside              2834 non-null   float64\n",
      " 4   Drained_after_socializing  2848 non-null   object \n",
      " 5   Friends_circle_size        2823 non-null   float64\n",
      " 6   Post_frequency             2835 non-null   float64\n",
      " 7   Personality                2900 non-null   object \n",
      "dtypes: float64(5), object(3)\n",
      "memory usage: 181.4+ KB\n",
      "None\n",
      "   Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
      "0               4.0         No                      4.0            6.0   \n",
      "1               9.0        Yes                      0.0            0.0   \n",
      "2               9.0        Yes                      1.0            2.0   \n",
      "3               0.0         No                      6.0            7.0   \n",
      "4               3.0         No                      9.0            4.0   \n",
      "\n",
      "  Drained_after_socializing  Friends_circle_size  Post_frequency Personality  \n",
      "0                        No                 13.0             5.0   Extrovert  \n",
      "1                       Yes                  0.0             3.0   Introvert  \n",
      "2                       Yes                  5.0             2.0   Introvert  \n",
      "3                        No                 14.0             8.0   Extrovert  \n",
      "4                        No                  8.0             5.0   Extrovert  \n",
      "        Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
      "count        2837.000000       2827              2838.000000    2834.000000   \n",
      "unique               NaN          2                      NaN            NaN   \n",
      "top                  NaN         No                      NaN            NaN   \n",
      "freq                 NaN       1417                      NaN            NaN   \n",
      "mean            4.505816        NaN                 3.963354       3.000000   \n",
      "std             3.479192        NaN                 2.903827       2.247327   \n",
      "min             0.000000        NaN                 0.000000       0.000000   \n",
      "25%             2.000000        NaN                 2.000000       1.000000   \n",
      "50%             4.000000        NaN                 3.000000       3.000000   \n",
      "75%             8.000000        NaN                 6.000000       5.000000   \n",
      "max            11.000000        NaN                10.000000       7.000000   \n",
      "\n",
      "       Drained_after_socializing  Friends_circle_size  Post_frequency  \\\n",
      "count                       2848          2823.000000     2835.000000   \n",
      "unique                         2                  NaN             NaN   \n",
      "top                           No                  NaN             NaN   \n",
      "freq                        1441                  NaN             NaN   \n",
      "mean                         NaN             6.268863        3.564727   \n",
      "std                          NaN             4.289693        2.926582   \n",
      "min                          NaN             0.000000        0.000000   \n",
      "25%                          NaN             3.000000        1.000000   \n",
      "50%                          NaN             5.000000        3.000000   \n",
      "75%                          NaN            10.000000        6.000000   \n",
      "max                          NaN            15.000000       10.000000   \n",
      "\n",
      "       Personality  \n",
      "count         2900  \n",
      "unique           2  \n",
      "top      Extrovert  \n",
      "freq          1491  \n",
      "mean           NaN  \n",
      "std            NaN  \n",
      "min            NaN  \n",
      "25%            NaN  \n",
      "50%            NaN  \n",
      "75%            NaN  \n",
      "max            NaN  \n",
      "Time_spent_Alone             63\n",
      "Stage_fear                   73\n",
      "Social_event_attendance      62\n",
      "Going_outside                66\n",
      "Drained_after_socializing    52\n",
      "Friends_circle_size          77\n",
      "Post_frequency               65\n",
      "Personality                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/personality_dataset.csv\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d75f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (2900, 7)\n",
      "Processed X shape: (2900, 9)\n",
      "Original y shape: (2900,)\n",
      "Encoded y shape: (2900,)\n",
      "Sample of encoded y: [0 1 1 0 0]\n",
      "Sample of processed X (first 5 rows, first 5 columns):\n",
      " [[ 4.  4.  6. 13.  5.]\n",
      " [ 9.  0.  0.  0.  3.]\n",
      " [ 9.  1.  2.  5.  2.]\n",
      " [ 0.  6.  7. 14.  8.]\n",
      " [ 3.  9.  4.  8.  5.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separate target variable\n",
    "X = df.drop(\"Personality\", axis=1)\n",
    "y = df[\"Personality\"]\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_cols = X.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "# Preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Create a preprocessor object using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, numerical_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Original X shape: {X.shape}\")\n",
    "print(f\"Processed X shape: {X_processed.shape}\")\n",
    "print(f\"Original y shape: {y.shape}\")\n",
    "print(f\"Encoded y shape: {y_encoded.shape}\")\n",
    "print(\"Sample of encoded y:\", y_encoded[:5])\n",
    "print(\"Sample of processed X (first 5 rows, first 5 columns):\\n\", X_processed[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp311-cp311-win_amd64.whl (10.7 MB)\n",
      "     ---------------------------------------- 10.7/10.7 MB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\pcadmin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.2.5)\n",
      "Collecting scipy>=1.8.0\n",
      "  Downloading scipy-1.15.3-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "     ---------------------------------------- 41.2/41.2 MB 4.5 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11587ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Best Hyperparameters: {'classifier__bootstrap': True, 'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 100}\n",
      "\n",
      "Training Set Evaluation:\n",
      "Accuracy: 0.9366\n",
      "Precision: 0.9368\n",
      "Recall: 0.9366\n",
      "F1-Score: 0.9366\n",
      "\n",
      "Testing Set Evaluation:\n",
      "Accuracy: 0.9293\n",
      "Precision: 0.9296\n",
      "Recall: 0.9293\n",
      "F1-Score: 0.9293\n",
      "Confusion Matrix:\n",
      " [[278  24]\n",
      " [ 17 261]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Define the preprocessor (update according to your pipeline)\n",
    "# Example: Assuming 'preprocessor' is already defined as part of your pipeline\n",
    "# preprocessor = ... \n",
    "\n",
    "# Hyperparameter grid for RandomForestClassifier\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "# Create the pipeline with preprocessing and classifier\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and the corresponding model\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nTesting Set Evaluation:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1-Score: {test_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", test_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48604ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (2320, 9)\n",
      "Test set shape: (580, 9)\n",
      "\n",
      "==================================================\n",
      "Tuning Random Forest...\n",
      "==================================================\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "Best parameters for Random Forest:\n",
      "  max_depth: 10\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "  n_estimators: 100\n",
      "Best cross-validation score: 0.9388\n",
      "\n",
      "==================================================\n",
      "Tuning Gradient Boosting...\n",
      "==================================================\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best parameters for Gradient Boosting:\n",
      "  learning_rate: 0.05\n",
      "  max_depth: 3\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "  n_estimators: 100\n",
      "Best cross-validation score: 0.9384\n",
      "\n",
      "==================================================\n",
      "Tuning SVM...\n",
      "==================================================\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best parameters for SVM:\n",
      "  C: 0.1\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "Best cross-validation score: 0.9388\n",
      "\n",
      "==================================================\n",
      "Tuning Logistic Regression...\n",
      "==================================================\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters for Logistic Regression:\n",
      "  C: 0.01\n",
      "  penalty: l1\n",
      "  solver: liblinear\n",
      "Best cross-validation score: 0.9388\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON\n",
      "============================================================\n",
      "Random Forest        | CV Score: 0.9388\n",
      "Gradient Boosting    | CV Score: 0.9384\n",
      "SVM                  | CV Score: 0.9388\n",
      "Logistic Regression  | CV Score: 0.9388\n",
      "\n",
      "Best performing model: Random Forest (CV Score: 0.9388)\n",
      "\n",
      "============================================================\n",
      "FINAL EVALUATION - Random Forest\n",
      "============================================================\n",
      "Test Accuracy: 0.9138\n",
      "Cross-validation Score: 0.9388\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Extrovert       0.94      0.89      0.91       298\n",
      "   Introvert       0.89      0.94      0.91       282\n",
      "\n",
      "    accuracy                           0.91       580\n",
      "   macro avg       0.91      0.91      0.91       580\n",
      "weighted avg       0.91      0.91      0.91       580\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "   1. Drained_after_socializing_No   | Importance: 0.2796\n",
      "   2. Stage_fear_Yes                 | Importance: 0.1766\n",
      "   3. Social_event_attendance        | Importance: 0.1556\n",
      "   4. Drained_after_socializing_Yes  | Importance: 0.1322\n",
      "   5. Stage_fear_No                  | Importance: 0.0881\n",
      "   6. Going_outside                  | Importance: 0.0588\n",
      "   7. Time_spent_Alone               | Importance: 0.0495\n",
      "   8. Post_frequency                 | Importance: 0.0324\n",
      "   9. Friends_circle_size            | Importance: 0.0272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Define models and their hyperparameter grids\n",
    "models_and_params = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "for model_name, model_info in models_and_params.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_info['model'],\n",
    "        param_grid=model_info['params'],\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'best_estimator': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'grid_search': grid_search\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Compare all models\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "best_model_name = None\n",
    "best_score = 0\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    cv_score = result['best_cv_score']\n",
    "    print(f\"{model_name:20} | CV Score: {cv_score:.4f}\")\n",
    "    \n",
    "    if cv_score > best_score:\n",
    "        best_score = cv_score\n",
    "        best_model_name = model_name\n",
    "\n",
    "print(f\"\\nBest performing model: {best_model_name} (CV Score: {best_score:.4f})\")\n",
    "\n",
    "# Evaluate the best model on test set\n",
    "best_model = results[best_model_name]['best_estimator']\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL EVALUATION - {best_model_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Cross-validation Score: {best_score:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "target_names = label_encoder.classes_\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(f\"\\nTop 10 Most Important Features:\")\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add numerical feature names\n",
    "    feature_names.extend(numerical_cols.tolist())\n",
    "    \n",
    "    # Add categorical feature names (after one-hot encoding)\n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "        feature_names.extend(cat_feature_names.tolist())\n",
    "    \n",
    "    # Sort features by importance\n",
    "    importance_indices = np.argsort(feature_importance)[::-1]\n",
    "    \n",
    "    for i in range(min(10, len(feature_names))):\n",
    "        idx = importance_indices[i]\n",
    "        print(f\"  {i+1:2d}. {feature_names[idx]:30} | Importance: {feature_importance[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20c4d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 Model Evaluation Summary – Random Forest\n",
      "============================================================\n",
      "✅ Test Accuracy : 0.9138\n",
      "🎯 F1 Score      : 0.9135\n",
      "🎯 Precision     : 0.8919\n",
      "🎯 Recall        : 0.9362\n",
      "🔍 Best CV Score : 0.9388\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pcadmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 128269 (\\N{LEFT-POINTING MAGNIFYING GLASS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "C:\\Users\\pcadmin\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 128269 (\\N{LEFT-POINTING MAGNIFYING GLASS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluate the selected best model\n",
    "best_model = results[best_model_name]['best_estimator']\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute detailed metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 🎯 Display Model Info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"📊 Model Evaluation Summary – {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✅ Test Accuracy : {acc:.4f}\")\n",
    "print(f\"🎯 F1 Score      : {f1:.4f}\")\n",
    "print(f\"🎯 Precision     : {prec:.4f}\")\n",
    "print(f\"🎯 Recall        : {recall:.4f}\")\n",
    "print(f\"🔍 Best CV Score : {results[best_model_name]['best_cv_score']:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 🔵 Confusion Matrix – Styled & Colored\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(f'🔍 Confusion Matrix - {best_model_name}', fontsize=15)\n",
    "sns.heatmap(cm, annot=True, fmt='d', linewidths=.5, cmap='Blues', square=True,\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f65ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Evaluation Metrics Table:\n",
      "\n",
      "   Metric  Score\n",
      " Accuracy 0.9138\n",
      " F1 Score 0.9135\n",
      "Precision 0.8919\n",
      "   Recall 0.9362\n",
      " CV Score 0.9388\n"
     ]
    }
   ],
   "source": [
    "# Tabular output of classification metrics\n",
    "import pandas as pd\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Metric\": ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'CV Score'],\n",
    "    \"Score\": [acc, f1, prec, recall, results[best_model_name]['best_cv_score']]\n",
    "})\n",
    "\n",
    "# Round values\n",
    "metrics_df['Score'] = metrics_df['Score'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Display nicely\n",
    "print(\"\\n📋 Evaluation Metrics Table:\\n\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89e2bce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (2320, 9)\n",
      "Test set shape: (580, 9)\n",
      "\n",
      "============================================================\n",
      "TUNING RANDOM FOREST\n",
      "============================================================\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "\n",
      "📊 Random Forest Results:\n",
      "   Best Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "   Cross-Validation Accuracy: 0.9388\n",
      "   Training Accuracy: 0.9427\n",
      "   Test Accuracy: 0.9138\n",
      "   Test Precision: 0.9149\n",
      "   Test Recall: 0.9138\n",
      "   Test F1-Score: 0.9138\n",
      "   Overfitting (Train-Test): 0.0289\n",
      "\n",
      "============================================================\n",
      "TUNING GRADIENT BOOSTING\n",
      "============================================================\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "\n",
      "📊 Gradient Boosting Results:\n",
      "   Best Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "   Cross-Validation Accuracy: 0.9384\n",
      "   Training Accuracy: 0.9388\n",
      "   Test Accuracy: 0.9172\n",
      "   Test Precision: 0.9183\n",
      "   Test Recall: 0.9172\n",
      "   Test F1-Score: 0.9172\n",
      "   Overfitting (Train-Test): 0.0216\n",
      "\n",
      "============================================================\n",
      "TUNING SVM\n",
      "============================================================\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "\n",
      "📊 SVM Results:\n",
      "   Best Parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "   Cross-Validation Accuracy: 0.9388\n",
      "   Training Accuracy: 0.9388\n",
      "   Test Accuracy: 0.9172\n",
      "   Test Precision: 0.9183\n",
      "   Test Recall: 0.9172\n",
      "   Test F1-Score: 0.9172\n",
      "   Overfitting (Train-Test): 0.0216\n",
      "\n",
      "============================================================\n",
      "TUNING LOGISTIC REGRESSION\n",
      "============================================================\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "📊 Logistic Regression Results:\n",
      "   Best Parameters: {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "   Cross-Validation Accuracy: 0.9388\n",
      "   Training Accuracy: 0.9388\n",
      "   Test Accuracy: 0.9172\n",
      "   Test Precision: 0.9183\n",
      "   Test Recall: 0.9172\n",
      "   Test F1-Score: 0.9172\n",
      "   Overfitting (Train-Test): 0.0216\n",
      "\n",
      "====================================================================================================\n",
      "COMPREHENSIVE MODEL COMPARISON\n",
      "====================================================================================================\n",
      "              Model  CV_Accuracy  Train_Accuracy  Test_Accuracy  Test_Precision  Test_Recall  Test_F1  Overfitting\n",
      "      Random Forest       0.9388          0.9427         0.9138          0.9149       0.9138   0.9138       0.0289\n",
      "  Gradient Boosting       0.9384          0.9388         0.9172          0.9183       0.9172   0.9172       0.0216\n",
      "                SVM       0.9388          0.9388         0.9172          0.9183       0.9172   0.9172       0.0216\n",
      "Logistic Regression       0.9388          0.9388         0.9172          0.9183       0.9172   0.9172       0.0216\n",
      "\n",
      "====================================================================================================\n",
      "BEST MODELS BY DIFFERENT CRITERIA\n",
      "====================================================================================================\n",
      "🏆 Best CV Accuracy:        Random Forest (0.9388)\n",
      "🏆 Best Test Accuracy:      Gradient Boosting (0.9172)\n",
      "🏆 Best F1-Score:           Gradient Boosting (0.9172)\n",
      "🏆 Least Overfitting:       Gradient Boosting (0.0216)\n",
      "\n",
      "====================================================================================================\n",
      "SELECTED MODEL: Gradient Boosting\n",
      "====================================================================================================\n",
      "Selected based on: Highest Test Accuracy\n",
      "Test Accuracy: 0.9172\n",
      "CV Accuracy: 0.9384\n",
      "F1-Score: 0.9172\n",
      "\n",
      "Optimized Parameters:\n",
      "  learning_rate: 0.05\n",
      "  max_depth: 3\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "  n_estimators: 100\n",
      "\n",
      "====================================================================================================\n",
      "DETAILED ANALYSIS - Gradient Boosting\n",
      "====================================================================================================\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Extrovert       0.94      0.90      0.92       298\n",
      "   Introvert       0.90      0.94      0.92       282\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.92      0.92      0.92       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[267  31]\n",
      " [ 17 265]]\n",
      "\n",
      "Class-wise Accuracy:\n",
      "  Extrovert: 0.8960\n",
      "  Introvert: 0.9397\n",
      "\n",
      "====================================================================================================\n",
      "MODEL RANKING BY DIFFERENT METRICS\n",
      "====================================================================================================\n",
      "\n",
      "🏅 Ranking by CV_Accuracy:\n",
      "  1. Random Forest        | 0.9388\n",
      "  2. SVM                  | 0.9388\n",
      "  3. Logistic Regression  | 0.9388\n",
      "  4. Gradient Boosting    | 0.9384\n",
      "\n",
      "🏅 Ranking by Test_Accuracy:\n",
      "  1. Gradient Boosting    | 0.9172\n",
      "  2. SVM                  | 0.9172\n",
      "  3. Logistic Regression  | 0.9172\n",
      "  4. Random Forest        | 0.9138\n",
      "\n",
      "🏅 Ranking by Test_Precision:\n",
      "  1. Gradient Boosting    | 0.9183\n",
      "  2. SVM                  | 0.9183\n",
      "  3. Logistic Regression  | 0.9183\n",
      "  4. Random Forest        | 0.9149\n",
      "\n",
      "🏅 Ranking by Test_Recall:\n",
      "  1. Gradient Boosting    | 0.9172\n",
      "  2. SVM                  | 0.9172\n",
      "  3. Logistic Regression  | 0.9172\n",
      "  4. Random Forest        | 0.9138\n",
      "\n",
      "🏅 Ranking by Test_F1:\n",
      "  1. Gradient Boosting    | 0.9172\n",
      "  2. SVM                  | 0.9172\n",
      "  3. Logistic Regression  | 0.9172\n",
      "  4. Random Forest        | 0.9138\n",
      "\n",
      "====================================================================================================\n",
      "FEATURE IMPORTANCE - Gradient Boosting\n",
      "====================================================================================================\n",
      "Top 15 Most Important Features:\n",
      "                      Feature  Importance\n",
      " Drained_after_socializing_No    0.497887\n",
      "Drained_after_socializing_Yes    0.428492\n",
      "               Post_frequency    0.049290\n",
      "          Friends_circle_size    0.007182\n",
      "             Time_spent_Alone    0.007137\n",
      "      Social_event_attendance    0.006614\n",
      "                Going_outside    0.003362\n",
      "                Stage_fear_No    0.000035\n",
      "               Stage_fear_Yes    0.000000\n",
      "\n",
      "====================================================================================================\n",
      "SAVING RESULTS\n",
      "====================================================================================================\n",
      "✅ Model comparison saved as 'model_comparison_results.csv'\n",
      "✅ Detailed results saved as 'detailed_model_results.pkl'\n",
      "✅ Model files saved:\n",
      "   - best_personality_model_pipeline.pkl\n",
      "   - best_personality_model.pkl\n",
      "   - preprocessor.pkl\n",
      "   - label_encoder.pkl\n",
      "✅ Comprehensive model info saved as 'comprehensive_model_info.pkl'\n",
      "\n",
      "====================================================================================================\n",
      "HYPERPARAMETER TUNING COMPLETE!\n",
      "====================================================================================================\n",
      "🏆 Selected Model: Gradient Boosting\n",
      "📊 Test Accuracy: 0.9172\n",
      "📊 F1-Score: 0.9172\n",
      "💾 All results and models saved successfully!\n",
      "\n",
      "================================================================================\n",
      "QUICK RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Random Forest:\n",
      "  Test Accuracy: 0.9138\n",
      "  CV Accuracy:   0.9388\n",
      "  F1-Score:      0.9138\n",
      "  Overfitting:   0.0289\n",
      "\n",
      "Gradient Boosting:\n",
      "  Test Accuracy: 0.9172\n",
      "  CV Accuracy:   0.9384\n",
      "  F1-Score:      0.9172\n",
      "  Overfitting:   0.0216\n",
      "\n",
      "SVM:\n",
      "  Test Accuracy: 0.9172\n",
      "  CV Accuracy:   0.9388\n",
      "  F1-Score:      0.9172\n",
      "  Overfitting:   0.0216\n",
      "\n",
      "Logistic Regression:\n",
      "  Test Accuracy: 0.9172\n",
      "  CV Accuracy:   0.9388\n",
      "  F1-Score:      0.9172\n",
      "  Overfitting:   0.0216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Define models and their hyperparameter grids\n",
    "models_and_params = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store detailed results\n",
    "detailed_results = {}\n",
    "accuracy_comparison = []\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "for model_name, model_info in models_and_params.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TUNING {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_info['model'],\n",
    "        param_grid=model_info['params'],\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate various metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    cv_accuracy = grid_search.best_score_\n",
    "    \n",
    "    # Calculate additional metrics (using weighted average for multiclass)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    # Store detailed results\n",
    "    detailed_results[model_name] = {\n",
    "        'best_estimator': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_accuracy': cv_accuracy,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1,\n",
    "        'y_pred': y_test_pred,\n",
    "        'grid_search': grid_search\n",
    "    }\n",
    "    \n",
    "    # Store for comparison table\n",
    "    accuracy_comparison.append({\n",
    "        'Model': model_name,\n",
    "        'CV_Accuracy': cv_accuracy,\n",
    "        'Train_Accuracy': train_accuracy,\n",
    "        'Test_Accuracy': test_accuracy,\n",
    "        'Test_Precision': test_precision,\n",
    "        'Test_Recall': test_recall,\n",
    "        'Test_F1': test_f1,\n",
    "        'Overfitting': train_accuracy - test_accuracy\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n📊 {model_name} Results:\")\n",
    "    print(f\"   Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"   Cross-Validation Accuracy: {cv_accuracy:.4f}\")\n",
    "    print(f\"   Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"   Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"   Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"   Test F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"   Overfitting (Train-Test): {train_accuracy - test_accuracy:.4f}\")\n",
    "\n",
    "# Create comprehensive comparison DataFrame\n",
    "comparison_df = pd.DataFrame(accuracy_comparison)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(f\"{'='*100}\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model based on different criteria\n",
    "best_cv_model = comparison_df.loc[comparison_df['CV_Accuracy'].idxmax(), 'Model']\n",
    "best_test_model = comparison_df.loc[comparison_df['Test_Accuracy'].idxmax(), 'Model']\n",
    "best_f1_model = comparison_df.loc[comparison_df['Test_F1'].idxmax(), 'Model']\n",
    "least_overfitting = comparison_df.loc[comparison_df['Overfitting'].idxmin(), 'Model']\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"BEST MODELS BY DIFFERENT CRITERIA\")\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"🏆 Best CV Accuracy:        {best_cv_model} ({comparison_df[comparison_df['Model']==best_cv_model]['CV_Accuracy'].values[0]:.4f})\")\n",
    "print(f\"🏆 Best Test Accuracy:      {best_test_model} ({comparison_df[comparison_df['Model']==best_test_model]['Test_Accuracy'].values[0]:.4f})\")\n",
    "print(f\"🏆 Best F1-Score:           {best_f1_model} ({comparison_df[comparison_df['Model']==best_f1_model]['Test_F1'].values[0]:.4f})\")\n",
    "print(f\"🏆 Least Overfitting:       {least_overfitting} ({comparison_df[comparison_df['Model']==least_overfitting]['Overfitting'].values[0]:.4f})\")\n",
    "\n",
    "# Select the best model (you can change the selection criteria)\n",
    "# Using test accuracy as the primary criterion\n",
    "selected_model_name = best_test_model\n",
    "selected_model_results = detailed_results[selected_model_name]\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"SELECTED MODEL: {selected_model_name}\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "selected_model = selected_model_results['best_estimator']\n",
    "selected_params = selected_model_results['best_params']\n",
    "\n",
    "print(f\"Selected based on: Highest Test Accuracy\")\n",
    "print(f\"Test Accuracy: {selected_model_results['test_accuracy']:.4f}\")\n",
    "print(f\"CV Accuracy: {selected_model_results['cv_accuracy']:.4f}\")\n",
    "print(f\"F1-Score: {selected_model_results['test_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimized Parameters:\")\n",
    "for param, value in selected_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Detailed analysis for selected model\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"DETAILED ANALYSIS - {selected_model_name}\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Classification report\n",
    "target_names = label_encoder.classes_\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, selected_model_results['y_pred'], target_names=target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, selected_model_results['y_pred'])\n",
    "print(cm)\n",
    "\n",
    "# Class-wise accuracy\n",
    "print(f\"\\nClass-wise Accuracy:\")\n",
    "for i, class_name in enumerate(target_names):\n",
    "    class_accuracy = cm[i, i] / cm[i, :].sum()\n",
    "    print(f\"  {class_name}: {class_accuracy:.4f}\")\n",
    "\n",
    "# Model comparison visualization data\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"MODEL RANKING BY DIFFERENT METRICS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "metrics = ['CV_Accuracy', 'Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1']\n",
    "for metric in metrics:\n",
    "    ranked = comparison_df.nlargest(len(comparison_df), metric)[['Model', metric]]\n",
    "    print(f\"\\n🏅 Ranking by {metric}:\")\n",
    "    for idx, (_, row) in enumerate(ranked.iterrows(), 1):\n",
    "        print(f\"  {idx}. {row['Model']:<20} | {row[metric]:.4f}\")\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(selected_model, 'feature_importances_'):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"FEATURE IMPORTANCE - {selected_model_name}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    feature_importance = selected_model.feature_importances_\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = []\n",
    "    feature_names.extend(numerical_cols.tolist())\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "        feature_names.extend(cat_feature_names.tolist())\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    feature_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 15 Most Important Features:\")\n",
    "    print(feature_df.head(15).to_string(index=False))\n",
    "\n",
    "# Save all results\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"SAVING RESULTS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"✅ Model comparison saved as 'model_comparison_results.csv'\")\n",
    "\n",
    "# Save detailed results\n",
    "with open('detailed_model_results.pkl', 'wb') as f:\n",
    "    pickle.dump(detailed_results, f)\n",
    "print(\"✅ Detailed results saved as 'detailed_model_results.pkl'\")\n",
    "\n",
    "# Create a complete pipeline with the selected model\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "complete_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', selected_model)\n",
    "])\n",
    "\n",
    "complete_pipeline.fit(X, y_encoded)\n",
    "\n",
    "# Save the selected model and components\n",
    "joblib.dump(complete_pipeline, 'best_personality_model_pipeline.pkl')\n",
    "joblib.dump(selected_model, 'best_personality_model.pkl')\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "print(\"✅ Model files saved:\")\n",
    "print(\"   - best_personality_model_pipeline.pkl\")\n",
    "print(\"   - best_personality_model.pkl\")\n",
    "print(\"   - preprocessor.pkl\")\n",
    "print(\"   - label_encoder.pkl\")\n",
    "\n",
    "# Save comprehensive model information\n",
    "model_info = {\n",
    "    'selected_model_name': selected_model_name,\n",
    "    'selection_criteria': 'Highest Test Accuracy',\n",
    "    'model_comparison': comparison_df.to_dict('records'),\n",
    "    'best_params': selected_params,\n",
    "    'performance_metrics': {\n",
    "        'cv_accuracy': selected_model_results['cv_accuracy'],\n",
    "        'test_accuracy': selected_model_results['test_accuracy'],\n",
    "        'test_precision': selected_model_results['test_precision'],\n",
    "        'test_recall': selected_model_results['test_recall'],\n",
    "        'test_f1': selected_model_results['test_f1']\n",
    "    },\n",
    "    'feature_names': feature_names if 'feature_names' in locals() else None,\n",
    "    'target_classes': target_names.tolist()\n",
    "}\n",
    "\n",
    "with open('comprehensive_model_info.pkl', 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "print(\"✅ Comprehensive model info saved as 'comprehensive_model_info.pkl'\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"🏆 Selected Model: {selected_model_name}\")\n",
    "print(f\"📊 Test Accuracy: {selected_model_results['test_accuracy']:.4f}\")\n",
    "print(f\"📊 F1-Score: {selected_model_results['test_f1']:.4f}\")\n",
    "print(f\"💾 All results and models saved successfully!\")\n",
    "\n",
    "# Function to display results summary\n",
    "def display_results_summary():\n",
    "    \"\"\"Display a quick summary of all model results\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"QUICK RESULTS SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for model_name, results in detailed_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "        print(f\"  CV Accuracy:   {results['cv_accuracy']:.4f}\")\n",
    "        print(f\"  F1-Score:      {results['test_f1']:.4f}\")\n",
    "        print(f\"  Overfitting:   {results['train_accuracy'] - results['test_accuracy']:.4f}\")\n",
    "\n",
    "# Call the summary function\n",
    "display_results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a7df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9224\n",
      "Precision: 0.9088\n",
      "Recall: 0.9317\n",
      "F1-Score: 0.9201\n",
      "Confusion Matrix:\n",
      " [[276  26]\n",
      " [ 19 259]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Create the full pipeline with preprocessing and model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00b95c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data prediction: Extrovert\n"
     ]
    }
   ],
   "source": [
    "# Example of making a prediction on new data\n",
    "# Let's create a sample new data point (ensure it has the same columns as the training data)\n",
    "new_data = pd.DataFrame([{\n",
    "    'Time_spent_Alone': 5,\n",
    "    'Stage_fear': 'No',\n",
    "    'Social_event_attendance': 3,\n",
    "    'Going_outside': 7,\n",
    "    'Drained_after_socializing': 'No',\n",
    "    'Friends_circle_size': 10,\n",
    "    'Post_frequency': 6\n",
    "}])\n",
    "\n",
    "# Make prediction\n",
    "new_prediction_encoded = best_model.predict(new_data)\n",
    "new_prediction_personality = label_encoder.inverse_transform(new_prediction_encoded)\n",
    "\n",
    "print(f\"New data prediction: {new_prediction_personality[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of making a prediction on new data\n",
    "# Let's create a sample new data point (ensure it has the same columns as the training data)\n",
    "new_data = pd.DataFrame([{\n",
    "    'Time_spent_Alone': 5,\n",
    "    'Stage_fear': 'No',\n",
    "    'Social_event_attendance': 3,\n",
    "    'Going_outside': 7,\n",
    "    'Drained_after_socializing': 'No',\n",
    "    'Friends_circle_size': 10,\n",
    "    'Post_frequency': 6\n",
    "}])\n",
    "\n",
    "# Make prediction\n",
    "new_prediction_encoded = best_model.predict(new_data)\n",
    "new_prediction_personality = label_encoder.inverse_transform(new_prediction_encoded)\n",
    "\n",
    "print(f\"New data prediction: {new_prediction_personality[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4a8b4",
   "metadata": {},
   "source": [
    "New Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8aaf8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data prediction: Extrovert\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.DataFrame([{\n",
    "    'Time_spent_Alone': 5,\n",
    "    'Stage_fear': 'No',\n",
    "    'Social_event_attendance': 3,\n",
    "    'Going_outside': 7,\n",
    "    'Drained_after_socializing': 'No',\n",
    "    'Friends_circle_size': 10,\n",
    "    'Post_frequency': 6\n",
    "}])\n",
    "\n",
    "# Ensure new_data has the same preprocessing applied\n",
    "new_data_preprocessed = preprocessor.transform(new_data)\n",
    "\n",
    "# Make prediction\n",
    "new_prediction_encoded = best_model.predict(new_data_preprocessed)\n",
    "new_prediction_personality = label_encoder.inverse_transform(new_prediction_encoded)\n",
    "\n",
    "print(f\"New data prediction: {new_prediction_personality[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f4d989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the entire dataset: 0.9352\n",
      "   Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
      "0               4.0         No                      4.0            6.0   \n",
      "1               9.0        Yes                      0.0            0.0   \n",
      "2               9.0        Yes                      1.0            2.0   \n",
      "3               0.0         No                      6.0            7.0   \n",
      "4               3.0         No                      9.0            4.0   \n",
      "\n",
      "  Drained_after_socializing  Friends_circle_size  Post_frequency Personality  \\\n",
      "0                        No                 13.0             5.0   Extrovert   \n",
      "1                       Yes                  0.0             3.0   Introvert   \n",
      "2                       Yes                  5.0             2.0   Introvert   \n",
      "3                        No                 14.0             8.0   Extrovert   \n",
      "4                        No                  8.0             5.0   Extrovert   \n",
      "\n",
      "   Predicted  \n",
      "0  Extrovert  \n",
      "1  Introvert  \n",
      "2  Introvert  \n",
      "3  Extrovert  \n",
      "4  Extrovert  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/personality_dataset.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=[\"Personality\"])  # Assuming \"Personality\" is the target column\n",
    "y_actual = df[\"Personality\"]\n",
    "\n",
    "# Encode the target variable (if not already encoded)\n",
    "# Assuming label_encoder was used earlier for encoding the \"Personality\" column\n",
    "y_encoded_actual = label_encoder.transform(y_actual)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "y_pred_encoded = best_model.predict(X)\n",
    "\n",
    "# Decode the predictions to original labels\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Add the predictions as a new column to the DataFrame\n",
    "df[\"Predicted\"] = y_pred\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"data/personality_dataset_with_predictions.csv\", index=False)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = accuracy_score(y_actual, y_pred)\n",
    "print(f\"Accuracy on the entire dataset: {accuracy:.4f}\")\n",
    "\n",
    "# Optionally, print the first few rows to verify the output\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30b6538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions saved to: data/personality_dataset_with_predictions2.csv\n",
      "\n",
      "📈 Model Evaluation on Full Dataset\n",
      "----------------------------------------\n",
      "✅ Accuracy: 0.9345\n",
      "\n",
      "🔍 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Extrovert       0.95      0.93      0.94      1491\n",
      "   Introvert       0.92      0.94      0.93      1409\n",
      "\n",
      "    accuracy                           0.93      2900\n",
      "   macro avg       0.93      0.93      0.93      2900\n",
      "weighted avg       0.93      0.93      0.93      2900\n",
      "\n",
      "\n",
      "🔢 Sample Predictions:\n",
      "  Personality  Predicted\n",
      "0   Extrovert  Extrovert\n",
      "1   Introvert  Introvert\n",
      "2   Introvert  Introvert\n",
      "3   Extrovert  Extrovert\n",
      "4   Extrovert  Extrovert\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# 📂 Load saved pipeline & label encoder\n",
    "pipeline = joblib.load('best_personality_model_pipeline.pkl')\n",
    "label_encoder = joblib.load('label_encoder.pkl')  # Used to decode prediction outputs\n",
    "\n",
    "# 📄 Load raw dataset (same format used during training)\n",
    "df = pd.read_csv(\"data/personality_dataset.csv\")\n",
    "\n",
    "# 🎯 Separate features and target\n",
    "X = df.drop(columns=[\"Personality\"])\n",
    "y_actual = df[\"Personality\"]\n",
    "\n",
    "# 🔁 Encode actual target using label encoder\n",
    "y_encoded_actual = label_encoder.transform(y_actual)\n",
    "\n",
    "# ✅ Predict using full pipeline with preprocessing included\n",
    "y_pred_encoded = pipeline.predict(X)\n",
    "\n",
    "# 🔁 Decode predictions back to original \"Introvert\"/\"Extrovert\"\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# ➕ Add predictions to the original DataFrame\n",
    "df[\"Predicted\"] = y_pred\n",
    "\n",
    "# 💾 Save to a new CSV\n",
    "output_path = \"data/personality_dataset_with_predictions2.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Predictions saved to: {output_path}\")\n",
    "\n",
    "# 📊 Evaluate accuracy & display classification report\n",
    "accuracy = accuracy_score(y_actual, y_pred)\n",
    "print(\"\\n📈 Model Evaluation on Full Dataset\")\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f}\\n\")\n",
    "print(\"🔍 Classification Report:\")\n",
    "print(classification_report(y_actual, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# 🧾 Sample preview\n",
    "print(\"\\n🔢 Sample Predictions:\")\n",
    "print(df[['Personality', 'Predicted']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
